{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.companies.processor import clean_company_type, normalize_company_name\n",
    "from src.companies.utils import replace_company_types\n",
    "from src.nif_validation.validation import (\n",
    "    get_nif_type,\n",
    "    validate_nif,\n",
    "    is_valid_nif,\n",
    "    # is_valid_cif,\n",
    "    # is_valid_dni,\n",
    "    # is_valid_nie,\n",
    "    get_info_from_cif,\n",
    ")\n",
    "from src.utils.utils import fill_to_length, merge_orig_dataframes\n",
    "from src.utils.utils_parallelization import (\n",
    "    parallelize_function,\n",
    "    parallelize_function_with_progress_bar,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\josea\\Downloads\\genCat_Junio_2023.json\", \"r\") as f:\n",
    "    gencat = pd.json_normalize(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(r\"C:\\Users\\josea\\Downloads\\Contractaci__p_blica_a_Catalunya__publicacions_a_la_Plataforma_de_serveis_de_contractaci__p_blica.csv\")\n",
    "df_emp = pd.read_csv(r\"C:\\Users\\josea\\Downloads\\empresas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data\n",
    "# dir_df = Path(\"C:/Users/josea/Documents/Trabajo/data/metadata/insiders.parquet\")\n",
    "# df_in = pd.read_parquet(dir_df)\n",
    "# dir_df = Path(\"C:/Users/josea/Documents/Trabajo/data/metadata/outsiders.parquet\")\n",
    "# df_ou = pd.read_parquet(dir_df)\n",
    "# dir_df = Path(\"C:/Users/josea/Documents/Trabajo/data/metadata/minors.parquet\")\n",
    "# df_mi = pd.read_parquet(dir_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_in.columns = [\".\".join([el for el in c if el]) for c in df_in.columns]\n",
    "# df_ou.columns = [\".\".join([el for el in c if el]) for c in df_ou.columns]\n",
    "# df_mi.columns = [\".\".join([el for el in c if el]) for c in df_mi.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ERROR A REVISAR\n",
    "# df_mi.loc[\n",
    "#     df_mi[\"id\"]\n",
    "#     .str.lower()\n",
    "#     .isin(\n",
    "#         [\n",
    "#             \"https://contrataciondelestado.es/sindicacion/datosabiertosmenores/8622601\",\n",
    "#             \"https://contrataciondelestado.es/sindicacion/datosabiertosmenores/8410165\",\n",
    "#         ]\n",
    "#     ),\n",
    "#     [\n",
    "#         \"ContractFolderStatus.TenderResult.WinningParty.PartyIdentification.ID\",\n",
    "#         \"ContractFolderStatus.TenderResult.WinningParty.PartyName.Name\",\n",
    "#     ],\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aux functions\n",
    "Functions necessary for processing the cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nif_from_name(name):\n",
    "    name_spl = np.array(name.split())\n",
    "    valid = np.array([bool(validate_nif(s)) for s in name_spl])\n",
    "    new_name = \" \".join(name_spl[~valid])\n",
    "    new_nif = Counter(name_spl[valid]).most_common()[0][0] if valid.any() else np.nan\n",
    "    return new_name, new_nif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def log_time(task_name: str):\n",
    "    \"\"\"Context manager to log the execution time of a block of code.\"\"\"\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    t1 = time.time()\n",
    "    print(f\"{task_name} - {t1-t0}\")\n",
    "\n",
    "\n",
    "def execute_function(func, data, prefer=None, workers=-1, *args, **kwargs):\n",
    "    \"\"\"Wrapper function to decide whether to use parallel processing or not.\"\"\"\n",
    "    if not prefer:\n",
    "        return data.apply(func, *args, **kwargs)\n",
    "    else:\n",
    "        return parallelize_function(\n",
    "            func, data, prefer=prefer, workers=workers, *args, **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "def clean_df(df: pd.DataFrame, prefer=None, workers=-1):\n",
    "    # Remove unwanted whitespace\n",
    "    with log_time(\"Removing unwanted whitespace\"):\n",
    "        df = df.applymap(\n",
    "            lambda x: regex.sub(r\"((?<=\\w+\\W)\\s+)|(\\s+(?=\\W\\w+))\", \"\", x)\n",
    "            if not pd.isna(x)\n",
    "            else None\n",
    "        )\n",
    "\n",
    "    # Validate NIF\n",
    "    with log_time(\"Validating NIF\"):\n",
    "        df[\"ID\"] = execute_function(validate_nif, df[\"ID\"], prefer, workers)\n",
    "\n",
    "    # Clean company type\n",
    "    with log_time(\"Cleaning company type\"):\n",
    "        name = [\n",
    "            regex.sub(i, \"\", n) if not (pd.isna(n) or pd.isna(i)) else n\n",
    "            for i, n in df[[\"ID\", \"Name\"]].values\n",
    "        ]\n",
    "        df[\"Name\"] = execute_function(\n",
    "            clean_company_type, name, prefer, workers, remove_type=False\n",
    "        )\n",
    "\n",
    "    # Remove company type\n",
    "    with log_time(\"Removing company type\"):\n",
    "        df[\"Name_proc\"] = execute_function(\n",
    "            clean_company_type, df[\"Name\"], prefer, workers, remove_type=True\n",
    "        )\n",
    "\n",
    "    # Normalize company name\n",
    "    with log_time(\"Normalizing company name\"):\n",
    "        df[\"Name_norm\"] = execute_function(\n",
    "            normalize_company_name, df[\"Name_proc\"], prefer, workers\n",
    "        )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge data from different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_companies = merge_orig_dataframes(\n",
    "#     dir_metadata=Path(\"C:/Users/josea/Documents/Trabajo/data/metadata/\")\n",
    "# )\n",
    "# df_companies.to_parquet(\"data/companies.parquet\")\n",
    "\n",
    "df_companies = pd.read_parquet(\"data/companies.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain individual companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use only those where all dimensions match\n",
    "# # (e.g. same number of companies and companies ids)\n",
    "# # and drop NAs\n",
    "# df_companies = df_companies[\n",
    "#     df_companies[[\"ID\", \"Name\"]]\n",
    "#     .applymap(lambda x: not pd.isna(x[0]))\n",
    "#     .apply(all, axis=1)\n",
    "# ]\n",
    "# df_companies = df_companies[\n",
    "#     df_companies.applymap(lambda x: len(x) if x[0] else None).apply(\n",
    "#         lambda x: len(set([el for el in x if not pd.isnull(el)])) == 1,\n",
    "#         axis=1,\n",
    "#     )\n",
    "# ]\n",
    "# companies_columns = list(df_companies.columns)\n",
    "# # Get number of companies by tender\n",
    "# df_companies[\"_len\"] = df_companies[\"ID\"].apply(len)\n",
    "\n",
    "# # Fill lists of None to have the same number of elements and explode later\n",
    "# companies = pd.DataFrame(\n",
    "#     df_companies.apply(\n",
    "#         lambda x: [fill_to_length(list(el), x[-1]) for el in x[:-1]], axis=1\n",
    "#     ).tolist(),\n",
    "#     columns=companies_columns,\n",
    "# )\n",
    "\n",
    "# # Split companies in rows\n",
    "# companies = companies.explode(companies_columns)\n",
    "# companies = companies.reset_index(drop=True)\n",
    "# display(companies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foreign/European IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# companies_foreign = companies[\n",
    "#     companies[\"ID\"].apply(lambda x: x[0].isalpha() and not is_valid_nif(x))\n",
    "# ]\n",
    "# companies_foreign[\"ID\"].apply(lambda x: x[:2]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# companies_foreign_valid = companies_foreign.loc[\n",
    "#     companies_foreign[\"ID\"].apply(lambda x: validate_nif(x[2:])).dropna().index\n",
    "# ]\n",
    "# companies_foreign_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean companies info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with log_time(\"Clean df\"):\n",
    "#     companies_clean = clean_df(companies, prefer=\"processes\", workers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Aggregate company info in lists\n",
    "# companies_clean[\"SMEAwardedIndicator\"] = companies_clean[\"SMEAwardedIndicator\"].apply(\n",
    "#     lambda x: None if not x else True if x == \"true\" else False\n",
    "# )\n",
    "# companies_clean = (\n",
    "#     companies_clean\n",
    "#     # companies[[\"ID\", \"Name\", \"Name_proc\", \"Name_norm\"]]\n",
    "#     .groupby([\"ID\", \"Name_norm\"])\n",
    "#     .agg(list)\n",
    "#     .reset_index()\n",
    "# )\n",
    "# companies_clean[\"count\"] = companies_clean[\"Name_proc\"].apply(len)\n",
    "# companies_clean = companies_clean.reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique names and IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Unique names and IDs\n",
    "# # These companies have always appeared with the same (id-name) association\n",
    "# cols_vals = [\n",
    "#     c for c in companies_clean.columns if c not in [\"ID\", \"Name_norm\", \"count\"]\n",
    "# ]\n",
    "# unique_ID = ~companies_clean[\"ID\"].duplicated(keep=False)\n",
    "# unique_NAME = ~companies_clean[\"Name_norm\"].duplicated(keep=False)\n",
    "\n",
    "# # Unique by ID and name\n",
    "# unique = companies_clean[unique_ID & unique_NAME]\n",
    "\n",
    "# # Non unique IDs\n",
    "# non_unique_ids = list(set(companies_clean[\"index\"]) - set(unique[\"index\"]))\n",
    "# non_unique = companies_clean[companies_clean[\"index\"].isin(non_unique_ids)]\n",
    "\n",
    "# unique[\"index\"] = unique[\"index\"].apply(lambda x: [x])\n",
    "# non_unique[\"index\"] = non_unique[\"index\"].apply(lambda x: [x])\n",
    "# print(unique.shape, non_unique.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeated IDs and Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose definitive values\n",
    "def suggest_value(elements):\n",
    "    \"\"\"\n",
    "    Select elements based on appearance.\n",
    "    If same number of appearances, choose the longest.\n",
    "    If shorter elements are not included in the 'main' one, return all.\n",
    "    \"\"\"\n",
    "    cnt = Counter(elements)\n",
    "    cnt.pop(None, None)\n",
    "    cnt = cnt.most_common()\n",
    "    if cnt:\n",
    "        max_cnt = cnt[0][1]\n",
    "        els = sorted([k for k, v in cnt if v == max_cnt], key=lambda x: (-len(x), x))\n",
    "        # return els[0]\n",
    "        base = els.pop(0)\n",
    "        return [base]\n",
    "        # if all(\n",
    "        #     [all(t in base for t in regex.sub(r\"\\W\", \" \", el).split()) for el in els]\n",
    "        # ):\n",
    "        #     return [base]\n",
    "        # return [base] + els\n",
    "    else:\n",
    "        # return None\n",
    "        return [None]\n",
    "\n",
    "\n",
    "# Repeated IDs\n",
    "def unify_repeated_col(df: pd.DataFrame, rep_col: str, un_col: str):\n",
    "    \"\"\"\n",
    "    Takes a dataframe with duplicated values in one column that should be unique (e.g. repeated IDs)\n",
    "    and another column that should also be unique given the previous one (e.g. title)\n",
    "    and unifies it so that it chooses the best option.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "    rep_col: str\n",
    "        Name of column with repeated values that will be unified\n",
    "    un_col: str\n",
    "        Name of column with non unique values\n",
    "    \"\"\"\n",
    "    # Non-unique columns\n",
    "    cols_vals = [c for c in df.columns if c not in [rep_col, \"count\", \"index\"]]\n",
    "    repeated_rows = df[rep_col].duplicated(keep=False)\n",
    "    repeated = df[repeated_rows]\n",
    "\n",
    "    # Count times the values appear\n",
    "    repeated.loc[repeated.index, [un_col]] = (\n",
    "        repeated.loc[repeated.index, un_col].apply(lambda x: [x])\n",
    "        * repeated.loc[repeated.index, \"count\"]\n",
    "    )\n",
    "    # Group by repeated\n",
    "    repeated = repeated.reset_index()\n",
    "    repeated = repeated.groupby(rep_col).agg(\n",
    "        {\n",
    "            # \"index\": list,\n",
    "            \"index\": sum,\n",
    "            **{c: lambda x: list(chain.from_iterable(x)) for c in cols_vals},\n",
    "            \"count\": sum,\n",
    "        }\n",
    "    )\n",
    "    # Get the most common values for each column\n",
    "    repeated.loc[repeated.index, un_col] = (\n",
    "        repeated.loc[repeated.index, un_col].apply(suggest_value).values\n",
    "    )\n",
    "    repeated = repeated.reset_index()\n",
    "\n",
    "    # Concatenate unique\n",
    "    use_index = repeated.loc[repeated[un_col].apply(len) == 1, un_col].index\n",
    "    repeated.loc[use_index, un_col] = repeated.loc[use_index, un_col].apply(\n",
    "        lambda x: x[0]\n",
    "    )\n",
    "    unified = repeated.loc[use_index]\n",
    "\n",
    "    return unified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Obtain unique ID-name\n",
    "# unified_ID = unify_repeated_col(non_unique, \"ID\", \"Name_norm\")\n",
    "# # Update non_unique\n",
    "# non_unique_ids = list(\n",
    "#     set(chain.from_iterable(non_unique[\"index\"]))\n",
    "#     - set(chain.from_iterable(unified_ID[\"index\"]))\n",
    "# )\n",
    "# # non_unique = companies_clean.loc[non_unique_ids]\n",
    "# non_unique = companies_clean[companies_clean[\"index\"].isin(non_unique_ids)]\n",
    "# non_unique[\"index\"] = non_unique[\"index\"].apply(lambda x: [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Obtain unique name-ID\n",
    "# unified_NAME = unify_repeated_col(non_unique, \"Name_norm\", \"ID\")\n",
    "# # Update non_unique\n",
    "# non_unique_ids = list(\n",
    "#     set(chain.from_iterable(non_unique[\"index\"]))\n",
    "#     - set(chain.from_iterable(unified_NAME[\"index\"]))\n",
    "# )\n",
    "# # non_unique = companies_clean.loc[non_unique_ids]\n",
    "# non_unique = companies_clean[companies_clean[\"index\"].isin(non_unique_ids)]\n",
    "# non_unique[\"index\"] = non_unique[\"index\"].apply(lambda x: [x])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Companies info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Global\n",
    "# # Merge unique+unifiedID+unifiedName+nonUnique\n",
    "# merged_global = pd.concat([unique, unified_ID, unified_NAME, non_unique])\n",
    "# cols_vals = [\n",
    "#     c\n",
    "#     for c in merged_global.columns\n",
    "#     if c not in [\"ID\", \"Name_norm\", \"count\", \"index\", \"id_tender\"]\n",
    "# ]\n",
    "# merged_global = merged_global.groupby([\"ID\", \"Name_norm\"]).agg(\n",
    "#     {\n",
    "#         # \"index\": lambda x: list(chain.from_iterable(x)),\n",
    "#         \"index\": sum,\n",
    "#         \"id_tender\": sum,\n",
    "#         **{c: lambda x: list(chain.from_iterable(x)) for c in cols_vals},\n",
    "#         \"count\": sum,\n",
    "#     }\n",
    "# )\n",
    "# merged_global = merged_global.reset_index()\n",
    "# print(len(merged_global))\n",
    "# display(merged_global.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unify found names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get all names found in the tenders\n",
    "# merged_global[\"UsedNames\"] = (merged_global[\"Name\"] + merged_global[\"Name_proc\"]).apply(\n",
    "#     lambda x: sorted(list(set(x)))\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propose a final name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initial computations\n",
    "# data = merged_global[\"Name_proc\"]\n",
    "# local_frequencies = data.apply(Counter)\n",
    "# proposed_names = local_frequencies.apply(lambda x: max(x, key=x.get))\n",
    "# # First proposal\n",
    "# merged_global[\"Name_proposed\"] = proposed_names\n",
    "\n",
    "# # Total names\n",
    "# global_freq_dict = data.explode().value_counts().to_dict()\n",
    "# names_list = list(global_freq_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def adjust_names_for_batch_v2(names_batch, local_frequencies, proposed_names):\n",
    "#     \"\"\"\n",
    "#     Adjusts and determines the most probable names for a given batch of names.\n",
    "\n",
    "#     For each name in the batch, the function checks if the name is associated\n",
    "#     with multiple IDs. If so, it retains the name only for the ID where the name\n",
    "#     has the highest frequency (local priority). For the other IDs, it assigns\n",
    "#     the next best name based on local frequency.\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     names_batch: List[str]\n",
    "#         A batch of names to be processed.\n",
    "#     local_frequencies: pd.Series\n",
    "#         Series containing the frequency count of each name for each ID.\n",
    "#     proposed_names: pd.Series\n",
    "#         Series mapping each ID to its currently assigned name.\n",
    "\n",
    "#     Returns:\n",
    "#     --------\n",
    "#     modified_real_names: Dict[int, str]:\n",
    "#         A dictionary with IDs as keys and their adjusted real names as values.\n",
    "#     \"\"\"\n",
    "#     modified_real_names = {}\n",
    "#     for name in names_batch:\n",
    "#         ids_with_name = proposed_names[proposed_names == name].index.tolist()\n",
    "#         if len(ids_with_name) > 1:\n",
    "#             ids_with_name.sort(key=lambda idx: -local_frequencies.loc[idx][name])\n",
    "#             for idx in ids_with_name[1:]:\n",
    "#                 del local_frequencies.loc[idx][name]\n",
    "#                 if local_frequencies.loc[idx]:\n",
    "#                     modified_real_names[idx] = max(\n",
    "#                         local_frequencies.loc[idx], key=local_frequencies.loc[idx].get\n",
    "#                     )\n",
    "\n",
    "#     return modified_real_names\n",
    "\n",
    "\n",
    "# new_proposed_name = parallelize_function_with_progress_bar(\n",
    "#     func=adjust_names_for_batch_v2,\n",
    "#     data=names_list,\n",
    "#     batch_size=1000,\n",
    "#     desc=\"progress_p2\",\n",
    "#     workers=-1,\n",
    "#     prefer=\"processes\",\n",
    "#     output=\"series\",\n",
    "#     local_frequencies=local_frequencies,\n",
    "#     proposed_names=proposed_names,\n",
    "# )\n",
    "\n",
    "\n",
    "# # Update the proposed names with the new ones\n",
    "# for item in new_proposed_name:\n",
    "#     merged_global.loc[item.keys(), \"Name_proposed\"] = list(item.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merged_global.to_parquet(\"data/merged_global.parquet\")\n",
    "# merged_global = pd.read_parquet(\"data/merged_global.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if company is SME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def isPYME(SMEIndicators):\n",
    "#     # Evaluate if is SME based on the SMEAwardedIndicator appearances\n",
    "#     # If True and False are present, return None\n",
    "#     # TODO: make a better decision\n",
    "#     sme_counts = Counter(SMEIndicators)\n",
    "#     if True in sme_counts and False in sme_counts:\n",
    "#         return None\n",
    "#     return sme_counts.most_common(1)[0][0]\n",
    "\n",
    "\n",
    "# merged_global[\"isPYME\"] = merged_global[\"SMEAwardedIndicator\"].apply(isPYME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check CityName and PostalZone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_city_name(CityName):\n",
    "#     # Evaluate the city name based on the CityName appearances\n",
    "#     # Get most common excluding None\n",
    "#     # TODO: make a better decision\n",
    "#     city_names = Counter(CityName)\n",
    "#     if None in city_names.keys():\n",
    "#         city_names.pop(None)\n",
    "#     if not len(city_names) == 1:\n",
    "#         return None\n",
    "#     return city_names.most_common(1)[0][0]\n",
    "\n",
    "\n",
    "# def get_postal_zone(PostalZone):\n",
    "#     # Evaluate the postal zone based on the PostalZone appearances\n",
    "#     # Get most common excluding None\n",
    "#     # TODO: make a better decision\n",
    "#     postal_zones = Counter(PostalZone)\n",
    "#     if None in postal_zones.keys():\n",
    "#         postal_zones.pop(None)\n",
    "#     if not len(postal_zones) == 1:\n",
    "#         return None\n",
    "#     return postal_zones.most_common(1)[0][0].split(\".\")[0]\n",
    "\n",
    "\n",
    "# merged_global[\"City\"] = merged_global[\"CityName\"].apply(get_city_name)\n",
    "# merged_global[\"PostalCode\"] = merged_global[\"PostalZone\"].apply(get_postal_zone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add information based on NIF\n",
    "# merged_global[\"NIF_type\"] = merged_global[\"ID\"].apply(get_nif_type)\n",
    "# merged_global[\"prov\"], merged_global[\"comp_type\"], merged_global[\"comp_desc\"] = list(\n",
    "#     zip(*merged_global[\"ID\"].apply(get_info_from_cif))\n",
    "# )\n",
    "# merged_global[\"comp_type\"] = merged_global[\"comp_type\"].apply(\n",
    "#     lambda x: x.split(\",\")[0] if not pd.isna(x) else None\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find UTEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find UTEs based on name\n",
    "# ute_n = provisional_company_info[\"UsedNames\"].apply(\n",
    "#     lambda x: bool(regex.search(r\"\\bu(\\.)?t(\\.)?e(\\.)?\\b\", \" \".join(x)))\n",
    "# )\n",
    "# # Find UTEs based on ID\n",
    "# ute_i = provisional_company_info[\"NIF\"].apply(lambda x: x.startswith(\"u\"))\n",
    "\n",
    "# # provisional_company_info[ute_i | ute_n][[\"NIF\", \"NameProposed\", \"UsedNames\"]]\n",
    "# # sum(ute_n), sum(ute_i), sum(ute_n & ute_i), sum(ute_n & ute_i)/min(sum(ute_n), sum(ute_i))\n",
    "\n",
    "# utes = provisional_company_info[ute_i | ute_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provisional_company_info = merged_global.rename(\n",
    "#     columns={\n",
    "#         \"ID\": \"NIF\",\n",
    "#         \"id_tender\": \"TenderAppearance\",\n",
    "#         \"prov\": \"Province\",\n",
    "#         \"NIF_type\": \"NIFtype\",\n",
    "#         \"comp_type\": \"CompanyType\",\n",
    "#         \"comp_desc\": \"CompanyDescription\",\n",
    "#         \"Name_proposed\": \"NameProposed\",\n",
    "#     }\n",
    "# )[\n",
    "#     [\n",
    "#         \"NIF\",\n",
    "#         \"NameProposed\",\n",
    "#         \"UsedNames\",\n",
    "#         \"Province\",\n",
    "#         \"City\",\n",
    "#         \"NIFtype\",\n",
    "#         \"CompanyType\",\n",
    "#         \"CompanyDescription\",\n",
    "#         \"isPYME\",\n",
    "#         \"TenderAppearance\",\n",
    "#     ]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provisional_company_info.to_parquet(\"data/provisional_company_info.parquet\")\n",
    "# utes.to_parquet(\"data/utes.parquet\")\n",
    "\n",
    "provisional_company_info = pd.read_parquet(\"data/provisional_company_info.parquet\")\n",
    "utes = pd.read_parquet(\"data/utes.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provisional_company_info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empresas Zaragoza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar las empresas de Zaragoza y darle el formato propio\n",
    "emp = pd.read_excel(\"data/licitador_09_23.xls\")\n",
    "emp_zgz = emp[[\"ID_EMPRESA\", \"NIF\", \"NOMBRE\", \"UTE\"]].copy()\n",
    "# Limpieza de nombres\n",
    "emp_zgz[\"empresa\"] = emp_zgz[\"NOMBRE\"].apply(clean_company_type)\n",
    "emp_zgz[\"empresa_proc\"] = emp_zgz[\"empresa\"].apply(clean_company_type, remove_type=True)\n",
    "# Creamos una columna con los nombres que se han usado para identificar la empresa\n",
    "emp_zgz[\"UsedNames\"] = emp_zgz[[\"empresa\", \"empresa_proc\"]].apply(\n",
    "    lambda x: list(set([x[0], x[1]])), axis=1\n",
    ")\n",
    "emp_zgz[\"nif\"] = emp_zgz[\"NIF\"].apply(\n",
    "    lambda x: regex.sub(r\"\\W\", \"\", x.lower()) if not pd.isna(x) else None\n",
    ")\n",
    "emp_zgz[\"valid_nif\"] = emp_zgz[\"nif\"].apply(lambda x: is_valid_nif(x) if x else None)\n",
    "emp_zgz[\"nif_type\"] = emp_zgz[\"nif\"].apply(lambda x: get_nif_type(x) if x else None)\n",
    "\n",
    "# Transformar UTE a bool\n",
    "emp_zgz[\"UTE\"] = emp_zgz[\"UTE\"].apply(\n",
    "    lambda x: True if x == \"S\" else False if x == \"N\" else None\n",
    ")\n",
    "\n",
    "# Stats\n",
    "print(\"Elementos inválidos en los datos:\")\n",
    "emp_zgz.apply(pd.isna, axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empresas repetidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_empresas = len(emp_zgz)\n",
    "\n",
    "# Identificar NIF repetidos\n",
    "emp_zgz_dup_nif = emp_zgz.loc[\n",
    "    emp_zgz[\"nif\"].duplicated(keep=False) & emp_zgz[\"nif\"].notna(),\n",
    "    [\"ID_EMPRESA\", \"nif\", \"empresa_proc\"],\n",
    "].sort_values(by=\"nif\")\n",
    "print(f\"Hay {len(emp_zgz_dup_nif)} NIFs duplicados:\")\n",
    "display(emp_zgz_dup_nif)\n",
    "print(\n",
    "    f\"Los NIFs duplicados representan el {(len(emp_zgz_dup_nif) / total_empresas) * 100:.2f}% del total.\"\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "# Identificar nombres repetidos\n",
    "emp_zgz_dup_name = emp_zgz.loc[\n",
    "    emp_zgz[\"empresa_proc\"].duplicated(keep=False) & emp_zgz[\"empresa_proc\"].notna(),\n",
    "    [\"ID_EMPRESA\", \"nif\", \"empresa_proc\"],\n",
    "].sort_values(by=\"empresa_proc\")\n",
    "print(f\"Hay {len(emp_zgz_dup_name)} nombres duplicados:\")\n",
    "display(emp_zgz_dup_name)\n",
    "print(\n",
    "    f\"Los nombres duplicados representan el {(len(emp_zgz_dup_name) / total_empresas) * 100:.2f}% del total.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search companies in own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_info = provisional_company_info[\n",
    "    [\"NIF\", \"UsedNames\", \"NameProposed\", \"isPYME\"]\n",
    "].rename(columns={\"NIF\": \"nif\"})\n",
    "own_info[\"UsedNames\"] = own_info[\"UsedNames\"].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar por NIF\n",
    "nifs_usados = own_info.reset_index()\n",
    "nif_zgz = emp_zgz[[\"ID_EMPRESA\", \"nif\", \"UTE\", \"empresa\", \"UsedNames\"]]\n",
    "found_by_nif = pd.merge(\n",
    "    nif_zgz, nifs_usados, left_on=\"nif\", right_on=\"nif\", how=\"inner\"\n",
    ")\n",
    "found_by_nif[\"UsedNames\"] = found_by_nif[[\"UsedNames_x\", \"UsedNames_y\"]].sum(axis=1)\n",
    "found_by_nif = (\n",
    "    found_by_nif.groupby(\"ID_EMPRESA\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"nif\": set,\n",
    "            \"index\": set,\n",
    "            \"UsedNames\": sum,\n",
    "            \"NameProposed\": set,\n",
    "            \"isPYME\": set,\n",
    "            \"UTE\": set,\n",
    "        }\n",
    "    )\n",
    "    .applymap(lambda x: list(set(x)))\n",
    "    .reset_index()\n",
    ")\n",
    "found_by_nif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# found_by_name.loc[found_by_name[\"UsedNames\"].apply(lambda x: \"reby rides\" in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar por nombre excluyendo las que hemos encontrado por NIF\n",
    "nombres_usados = own_info.explode(column=\"UsedNames\").reset_index()\n",
    "nombres_zgz = emp_zgz[~emp_zgz[\"ID_EMPRESA\"].isin(found_by_nif[\"ID_EMPRESA\"])]\n",
    "nombres_zgz = nombres_zgz[[\"ID_EMPRESA\", \"nif\", \"UTE\", \"empresa\", \"UsedNames\"]].explode(\n",
    "    column=\"UsedNames\"\n",
    ")\n",
    "found_by_name = pd.merge(\n",
    "    nombres_zgz, nombres_usados, left_on=\"UsedNames\", right_on=\"UsedNames\", how=\"inner\"\n",
    ")\n",
    "found_by_name[\"nif\"] = (\n",
    "    found_by_name[[\"nif_x\", \"nif_y\"]]\n",
    "    .applymap(validate_nif)\n",
    "    .apply(lambda x: list(set([el for el in x if el])), axis=1)\n",
    ")\n",
    "found_by_name = (\n",
    "    found_by_name.groupby(\"ID_EMPRESA\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"nif\": sum,\n",
    "            \"index\": set,\n",
    "            \"UsedNames\": set,\n",
    "            \"NameProposed\": set,\n",
    "            \"isPYME\": set,\n",
    "            \"UTE\": set,\n",
    "        }\n",
    "    )\n",
    "    .applymap(lambda x: list(set(x)))\n",
    "    .reset_index()\n",
    ")\n",
    "found_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unimos los dos dataframes\n",
    "found = (\n",
    "    pd.concat([found_by_nif, found_by_name])\n",
    "    .groupby(\"ID_EMPRESA\")\n",
    "    .agg(sum)\n",
    "    .applymap(lambda x: list(set(x)) if x else None)\n",
    "    .reset_index()\n",
    ")\n",
    "not_found = emp_zgz[~emp_zgz[\"ID_EMPRESA\"].isin(found[\"ID_EMPRESA\"])]\n",
    "# Stats\n",
    "print(f\"Se han encontrado {len(found)} empresas de Zaragoza en las empresas previas\")\n",
    "print(\"Elementos inválidos en los datos:\")\n",
    "print(found.apply(pd.isna, axis=1).sum().to_dict())\n",
    "print(f\"Hay {len(not_found)} empresas que no se han encontrado\")\n",
    "# found[[\"nif\", \"index\", \"UsedNames\", \"NameProposed\", \"isPYME\", \"UTE\"]].applymap(\n",
    "#     lambda x: x[0]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis empresas encontradas y no encontradas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorrect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_id_emp_ids = emp_zgz[emp_zgz[\"nif\"].isna()][\"ID_EMPRESA\"].values\n",
    "\n",
    "print(f\"Null ids: {len(null_id_emp_ids)}\")\n",
    "print(f\"Null ids found: {found['ID_EMPRESA'].isin(null_id_emp_ids).sum()}\")\n",
    "print(f\"Null ids not found: {not_found['ID_EMPRESA'].isin(null_id_emp_ids).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"De las empresas que no se han encontrado ({len(not_found)})\")\n",
    "not_found_valid = not_found[not_found[\"valid_nif\"].apply(bool)]\n",
    "not_found_invalid = not_found[~not_found[\"valid_nif\"].apply(bool)].copy()\n",
    "print(\n",
    "    f\"Hay {len(not_found_valid)} que tienen un NIF válido y {len(not_found_invalid)} que no\"\n",
    ")\n",
    "\n",
    "# Intento de corrección del nif\n",
    "not_found_invalid[\"proposed_nif\"] = (\n",
    "    not_found_invalid[\"nif\"].dropna().apply(validate_nif, correct=True)\n",
    ")\n",
    "print(f\"De los no encontrados, que tienen NIF no nulo, se han podido corregir:\")\n",
    "not_found_invalid.dropna(subset=\"proposed_nif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas que deberían tener valores únicos\n",
    "unique_columns = [c for c in found.columns if c not in [\"ID_EMPRESA\", \"UsedNames\"]]\n",
    "found[unique_columns] = found[unique_columns].applymap(\n",
    "    lambda x: [el for el in x if not pd.isna(el)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontradas\n",
    "unique = (\n",
    "    found[unique_columns]\n",
    "    .applymap(lambda x: len(x) if x else 1)\n",
    "    .apply(lambda x: all(x == 1), axis=1)\n",
    ")\n",
    "# Válidas (todo son valores únicos)\n",
    "valid_unique = found[unique].copy()\n",
    "valid_unique[unique_columns] = valid_unique[unique_columns].applymap(\n",
    "    lambda x: x[0] if x else None\n",
    ")\n",
    "\n",
    "# Inválidas (algún valor que debería ser único no lo es)\n",
    "invalid_unique = found[~unique].copy()\n",
    "# invalid_unique[unique_columns] = invalid_unique[unique_columns].applymap(lambda x: x if x else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Para Zaragoza, podemos sugerir un NIF en función de si la provincia es Zaragoza:\n",
    "# invalid_unique[\"nif\"].apply(\n",
    "#     lambda x: [get_info_from_cif(el)[0] == \"Zaragoza\" for el in x]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UTEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_zgz.loc[emp_zgz[\"UTE\"] == True, \"empresa\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_unique[\"UTE\"].value_counts().to_dict())\n",
    "print(invalid_unique[\"UTE\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found[\"UTE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_unique[valid_unique[\"UTE\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empresas no encontradas en datos propios, con NIF válido\n",
    "not_found = emp_zgz.loc[\n",
    "    ~emp_zgz[\"ID_EMPRESA\"].isin(found[\"ID_EMPRESA\"]) & emp_zgz[\"valid_nif\"].apply(bool)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rellenar NIF:\n",
    "# empty_nif = emp_zgz[\n",
    "#     (emp_zgz[\"ID_EMPRESA\"].isin(found[\"ID_EMPRESA\"])) & (emp_zgz[\"nif\"].isna())\n",
    "# ]\n",
    "# pd.merge(\n",
    "#     empty_nif[[\"ID_EMPRESA\", \"empresa\", \"empresa_proc\"]],\n",
    "#     found,\n",
    "#     left_on=\"ID_EMPRESA\",\n",
    "#     right_on=\"ID_EMPRESA\",\n",
    "#     how=\"inner\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df = pd.read_csv(r\"C:\\Users\\josea\\Downloads\\empresas.csv\", sep=\";\", header=0, nrows=64, index_col=False)\n",
    "# # df = pd.read_excel(r\"C:\\Users\\josea\\Downloads\\empresas.xlsx\")\n",
    "\n",
    "# with open(r\"C:\\Users\\josea\\Downloads\\empresas_zgz.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     emp = [\n",
    "#         [el.replace('\"', \"\").strip() for el in l.lower().strip().split(\";\", 4)]\n",
    "#         for l in f.readlines()\n",
    "#         if len(l) > 2\n",
    "#     ]\n",
    "# cols = emp[0]\n",
    "# data = emp[1:]\n",
    "# emp_zgz = pd.DataFrame(data=data, columns=cols)\n",
    "# emp_zgz = emp_zgz.applymap(lambda x: x if x else None)\n",
    "# emp_zgz = emp_zgz.dropna(how=\"all\").drop_duplicates().reset_index(drop=True)\n",
    "# emp_zgz[\"empresa\"] = emp_zgz[\"empresa\"].apply(clean_company_type)\n",
    "# emp_zgz[\"empresa_proc\"] = emp_zgz[\"empresa\"].apply(clean_company_type, remove_type=True)\n",
    "# emp_zgz[\"nif\"] = emp_zgz[\"nif\"].apply(lambda x: regex.sub(r\"\\W\", \"\", x) if x else None)\n",
    "# emp_zgz[\"nif_type\"] = emp_zgz[\"nif\"].apply(lambda x: get_nif_type(x) if x else None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
